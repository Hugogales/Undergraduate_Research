{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Team_1 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_2 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_3 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_4 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_5 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_6 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Match Results:\n",
      "    Team1   Team2  Winner\n",
      "0  Team_1  Team_6  Team_6\n",
      "\n",
      "Final Team Rankings:\n",
      "     Team  Rating_Mean  Rating_StdDev  TrueSkill_Score\n",
      "5  Team_6  1370.000049     371.474315       255.577104\n",
      "0  Team_1   629.999951     371.474315      -484.422994\n",
      "1  Team_2  1000.000000     500.000000      -500.000000\n",
      "2  Team_3  1000.000000     500.000000      -500.000000\n",
      "3  Team_4  1000.000000     500.000000      -500.000000\n",
      "4  Team_5  1000.000000     500.000000      -500.000000\n"
     ]
    }
   ],
   "source": [
    "# Install the trueskill library if you haven't already\n",
    "# !pip install trueskill\n",
    "\n",
    "import trueskill\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the TrueSkill environment\n",
    "ts = trueskill.TrueSkill(draw_probability=0.1, mu = 1000, sigma = 500)  # Adjust draw probability as needed\n",
    "\n",
    "# Define the number of teams and matches\n",
    "NUM_TEAMS = 6\n",
    "NUM_MATCHES = 1\n",
    "\n",
    "# Initialize teams with default ratings\n",
    "teams = {}\n",
    "for i in range(1, NUM_TEAMS + 1):\n",
    "    team_name = f\"Team_{i}\"\n",
    "    teams[team_name] = ts.Rating()\n",
    "    print(f\"Initialized {team_name} with rating: {teams[team_name]}\")\n",
    "\n",
    "# Function to simulate a match between two teams\n",
    "def simulate_match(ts_env, team1, team2):\n",
    "    # Get the current ratings\n",
    "    rating1 = teams[team1]\n",
    "    rating2 = teams[team2]\n",
    "    \n",
    "    # Calculate each team's performance by sampling from their rating distributions\n",
    "    perf1 = ts_env.expose(rating1)\n",
    "    perf2 = ts_env.expose(rating2)\n",
    "    \n",
    "    # Determine the outcome\n",
    "    if perf1 > perf2:\n",
    "        # Team1 wins\n",
    "        ranks = [0, 1]\n",
    "    elif perf1 < perf2:\n",
    "        # Team2 wins\n",
    "        ranks = [1.2, 0]\n",
    "    else:\n",
    "        # Draw\n",
    "        ranks = [1, 0]\n",
    "\n",
    "    \n",
    "    # Update the ratings\n",
    "    new_ratings = ts_env.rate([ [rating1], [rating2] ], ranks)\n",
    "    new_ratings = ts_env.rate([ [new_ratings[0][0]], [new_ratings[1][0]] ], ranks)\n",
    "    teams[team1] = new_ratings[0][0]\n",
    "    teams[team2] = new_ratings[1][0]\n",
    "    \n",
    "    # Return the outcome\n",
    "    return {\n",
    "        'Team1': team1,\n",
    "        'Team2': team2,\n",
    "        'Team1_Rating': teams[team1],\n",
    "        'Team2_Rating': teams[team2],\n",
    "        'Winner': team1 if ranks[0] == 0 and ranks[1] == 1 else (team2 if ranks[0] == 1 and ranks[1] == 0 else 'Draw')\n",
    "    }\n",
    "\n",
    "# Simulate matches\n",
    "match_results = []\n",
    "for _ in range(NUM_MATCHES):\n",
    "    # Randomly select two different teams\n",
    "    team1, team2 = random.sample(list(teams.keys()), 2)\n",
    "    result = simulate_match(ts, team1, team2)\n",
    "    match_results.append(result)\n",
    "\n",
    "# Create a DataFrame to display the match results\n",
    "df_matches = pd.DataFrame(match_results)\n",
    "\n",
    "# Calculate final ratings and confidence intervals\n",
    "final_ratings = []\n",
    "for team, rating in teams.items():\n",
    "    # The rating object has a mean (mu) and standard deviation (sigma)\n",
    "    # The conservative estimate of skill is called the \"TrueSkill\" score: mu - 3*sigma\n",
    "    trueskill_score = rating.mu - 3 * rating.sigma\n",
    "    final_ratings.append({\n",
    "        'Team': team,\n",
    "        'Rating_Mean': rating.mu,\n",
    "        'Rating_StdDev': rating.sigma,\n",
    "        'TrueSkill_Score': trueskill_score\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to display the final team ratings\n",
    "df_ratings = pd.DataFrame(final_ratings)\n",
    "df_ratings.sort_values(by='TrueSkill_Score', ascending=False, inplace=True)\n",
    "\n",
    "# Display the match results\n",
    "print(\"Match Results:\")\n",
    "print(df_matches[['Team1', 'Team2', 'Winner']])\n",
    "\n",
    "# Display the final team rankings\n",
    "print(\"\\nFinal Team Rankings:\")\n",
    "print(df_ratings[['Team', 'Rating_Mean', 'Rating_StdDev', 'TrueSkill_Score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Team_1 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_2 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_3 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_4 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_5 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "Initialized Team_6 with rating: trueskill.Rating(mu=1000.000, sigma=500.000)\n",
      "5 0\n",
      "5\n",
      "5\n",
      "5\n",
      "5.0\n",
      "Goal difference: 5\n",
      "\n",
      "Match Results:\n",
      "    Team1  Goals1   Team2  Goals2  Winner\n",
      "0  Team_1       5  Team_3       0  Team_1\n",
      "\n",
      "Final Team Rankings:\n",
      "     Team  Rating_Mean  Rating_StdDev  TrueSkill_Score\n",
      "0  Team_1  1466.516875     310.222023       535.850807\n",
      "2  Team_3   533.483125     310.222023      -397.182942\n",
      "1  Team_2  1000.000000     500.000000      -500.000000\n",
      "3  Team_4  1000.000000     500.000000      -500.000000\n",
      "4  Team_5  1000.000000     500.000000      -500.000000\n",
      "5  Team_6  1000.000000     500.000000      -500.000000\n"
     ]
    }
   ],
   "source": [
    "import trueskill\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the TrueSkill environment\n",
    "ts = trueskill.TrueSkill(draw_probability=0.1, mu=1000, sigma=100)\n",
    "\n",
    "# Define the number of teams and matches\n",
    "NUM_TEAMS = 6\n",
    "NUM_MATCHES = 100 # Increase the number of matches for clearer results\n",
    "\n",
    "# Initialize teams with default ratings\n",
    "teams = {}\n",
    "for i in range(1, NUM_TEAMS + 1):\n",
    "    team_name = f\"Team_{i}\"\n",
    "    teams[team_name] = ts.Rating()\n",
    "    print(f\"Initialized {team_name} with rating: {teams[team_name]}\")\n",
    "\n",
    "def simulate_match(ts_env, team1, team2, goals_team1, goals_team2):\n",
    "    rating1 = teams[team1]\n",
    "    rating2 = teams[team2]\n",
    "\n",
    "    # Determine the outcome for standard TrueSkill ranks\n",
    "    if goals_team1 > goals_team2:\n",
    "        ranks = [0, 1]\n",
    "    elif goals_team1 < goals_team2:\n",
    "        ranks = [1, 0]\n",
    "    else:\n",
    "        ranks = [0, 0]  # Let TrueSkill interpret a draw\n",
    "    \n",
    "    # Use margin of victory as extra updates\n",
    "    print(goals_team1, goals_team2)\n",
    "    goal_diff = abs(goals_team1 - goals_team2)\n",
    "    print(goal_diff)\n",
    "    max_goal = max(goals_team1, goals_team2)\n",
    "    print(max_goal)\n",
    "    print(sum([goals_team1, goals_team2]))\n",
    "    print(goal_diff * max_goal / sum([goals_team1, goals_team2]))\n",
    "    sum_goals = sum([goals_team1, goals_team2]) if sum([goals_team1, goals_team2]) > 0 else 1\n",
    "    goal_diff = round(goal_diff * max_goal / sum_goals) # Normalize to [0, 1]\n",
    "    print(f\"Goal difference: {goal_diff}\")\n",
    "\n",
    "    for _ in range(1 + goal_diff):\n",
    "        new_ratings = ts_env.rate([[rating1], [rating2]], ranks)\n",
    "        rating1, rating2 = new_ratings[0][0], new_ratings[1][0]\n",
    "\n",
    "    teams[team1], teams[team2] = rating1, rating2\n",
    "\n",
    "    return {\n",
    "        'Team1': team1,\n",
    "        'Team2': team2,\n",
    "        'Goals1': goals_team1,\n",
    "        'Goals2': goals_team2,\n",
    "        'Team1_Rating': rating1,\n",
    "        'Team2_Rating': rating2,\n",
    "        'Winner': team1 if goals_team1 > goals_team2 else (team2 if goals_team2 > goals_team1 else 'Draw')\n",
    "    }\n",
    "\n",
    "# Simulate matches with random goals\n",
    "match_results = []\n",
    "for _ in range(NUM_MATCHES):\n",
    "    team1, team2 = random.sample(list(teams.keys()), 2)\n",
    "    goals1 = random.randint(0, 5)\n",
    "    goals2 = random.randint(0, 5)\n",
    "    goals1 = 5\n",
    "    goals2 = 0\n",
    "    result = simulate_match(ts, team1, team2, goals1, goals2)\n",
    "    match_results.append(result)\n",
    "\n",
    "# Show match results\n",
    "df_matches = pd.DataFrame(match_results)\n",
    "print(\"\\nMatch Results:\")\n",
    "print(df_matches[['Team1', 'Goals1', 'Team2', 'Goals2', 'Winner']])\n",
    "\n",
    "# Final ratings and confidence intervals\n",
    "final_ratings = []\n",
    "for team, rating in teams.items():\n",
    "    trueskill_score = rating.mu - 3 * rating.sigma  # Conservative skill estimate\n",
    "    final_ratings.append({\n",
    "        'Team': team,\n",
    "        'Rating_Mean': rating.mu,\n",
    "        'Rating_StdDev': rating.sigma,\n",
    "        'TrueSkill_Score': trueskill_score\n",
    "    })\n",
    "\n",
    "df_ratings = pd.DataFrame(final_ratings)\n",
    "df_ratings.sort_values('TrueSkill_Score', ascending=False, inplace=True)\n",
    "\n",
    "print(\"\\nFinal Team Rankings:\")\n",
    "print(df_ratings[['Team', 'Rating_Mean', 'Rating_StdDev', 'TrueSkill_Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
